{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import evaluate\n",
    "from transformers import (GPT2Tokenizer,\n",
    "                          AutoTokenizer,\n",
    "                          GPT2LMHeadModel,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          AutoModelForCausalLM,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline,\n",
    "                          logging)\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from trl import SFTTrainer\n",
    "# from trl import setup_chat_format\n",
    "\n",
    "# import bitsandbytes\n",
    "import os\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             f1_score,\n",
    "                             classification_report,\n",
    "                             confusion_matrix,\n",
    "                             auc)\n",
    "\n",
    "from huggingface_hub import login\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, rgb2hex\n",
    "from matplotlib.ticker import PercentFormatter \n",
    "from IPython.display import HTML\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"pytorch version {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def random_seed(seed_value, use_cuda):\n",
    "    np.random.seed(seed_value)  # CPU vars\n",
    "    torch.manual_seed(seed_value)  # CPU vars\n",
    "    random.seed(seed_value)  # Python random seed\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)  # GPU vars\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def format_duration(total_time):\n",
    "    time_delta = timedelta(seconds=total_time)\n",
    "    hours, remainder = divmod(time_delta.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return \"{} hours, {} minutes, {} seconds\".format(hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " # Set the seed\n",
    "SEED = 42\n",
    "random_seed(SEED, torch.cuda.is_available())\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class HardDataset:\n",
    "    def __init__(self, data_path, seed=SEED):\n",
    "        self.data_path=data_path\n",
    "        self.seed=SEED\n",
    "        self.df=None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the data from a TSV file and retain only the 'rating' and 'review' columns.\n",
    "        Also, transform the 'rating' to a binary label (positive/negative).\n",
    "        \"\"\"\n",
    "        # Load the dataset\n",
    "        self.df = pd.read_csv(self.data_path, delimiter='\\t')\n",
    "\n",
    "        # Keep only 'rating' and 'review' columns\n",
    "        self.df = self.df[['rating', 'review']]\n",
    "\n",
    "        # Code rating: positive (1) if rating > 3, negative (0) if rating < 3\n",
    "        self.df['rating'] = self.df['rating'].apply(lambda x: 0 if x < 3 else 1)\n",
    "\n",
    "        # Rename columns for consistency with standard text classification format\n",
    "        self.df.columns = ['label', 'text']\n",
    "        print(f\"Initial dataset length: {len(self.df)}\")\n",
    "        return self.df\n",
    "\n",
    "    def analyze_data(self):\n",
    "        \"\"\"\n",
    "        Analyze the dataset: number of words,\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not loaded. Please call 'load_data()' first.\")\n",
    "\n",
    "        # Show label distribution\n",
    "        label_counts = self.df['label'].value_counts()\n",
    "        print(\"\\nLabel Distribution:\")\n",
    "        print(label_counts)\n",
    "\n",
    "        # Visualize label distribution\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.barplot(x=label_counts.index, y=label_counts.values, palette=\"viridis\")\n",
    "        plt.title(\"Label Distribution\")\n",
    "        plt.xlabel(\"Label\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "        plt.show()\n",
    "\n",
    "        self.df[\"word_count\"] = self.df.text.apply(lambda x:len(x.split(\" \")))\n",
    "\n",
    "        plt.hist(\n",
    "            self.df.word_count, weights=np.ones(len(self.df.word_count)) / len(self.df.word_count), bins=30\n",
    "         )\n",
    "        plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "        plt.xlabel(\"Words\")\n",
    "        plt.ylabel(\"Percentage\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.show()\n",
    "\n",
    "        print(self.df.shape)\n",
    "        # Delete instances with words greater than 100\n",
    "        len(self.df[self.df.word_count < 100]) / len(self.df) # 98% of examples left\n",
    "\n",
    "        # Delete\n",
    "        self.df = self.df[self.df.word_count < 100]\n",
    "        print(self.df.shape)\n",
    "\n",
    "        # Get maximum number of samples from each category to reduce resources\n",
    "        max_samples = 7000\n",
    "        # df = df.sample(frac=1, random_state=85).reset_index(drop=True).head(3000) # check if data is shuffled\n",
    "        df_sampled = self.df.groupby(\"label\")[[\"text\", \"label\"]].apply(\n",
    "            lambda x: x.sample(n=min(len(x), max_samples))\n",
    "        )\n",
    "        df_sampled = df_sampled.reset_index(drop=True)\n",
    "\n",
    "        df_sampled['text_length'] = df_sampled['text'].apply(len)\n",
    "        # Visualize text length distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df_sampled['text_length'], bins=30, kde=True, color=\"blue\")\n",
    "        plt.title(\"Text Length Distribution\")\n",
    "        plt.xlabel(\"Text Length (characters)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize boxplot of text lengths for each label\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=\"label\", y=\"text_length\", data=df_sampled, palette=\"coolwarm\")\n",
    "        plt.title(\"Text Length Distribution by Label\")\n",
    "        plt.xlabel(\"Label\")\n",
    "        plt.ylabel(\"Text Length (characters)\")\n",
    "        plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "        plt.show()\n",
    "\n",
    "        df_sampled = df_sampled.drop('text_length', axis=1)\n",
    "        # Split the data\n",
    "        # create train and test set\n",
    "        train_df, temp_df = train_test_split(\n",
    "            df_sampled, test_size=0.2, random_state=SEED\n",
    "        )\n",
    "\n",
    "        val_df, test_df = train_test_split(\n",
    "            temp_df, test_size=0.5, random_state=SEED\n",
    "        )\n",
    "\n",
    "        print(f\"Train shape: {train_df.shape}\")\n",
    "        print(f\"Validation shape: {val_df.shape}\")\n",
    "        print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "        return df_sampled, train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hardDataset = HardDataset(\"/kaggle/input/sa-hard-arabic/balanced-reviews-utf8.tsv\")\n",
    "df = hardDataset.load_data()\n",
    "df_sampled, train_df, val_df, test_df = hardDataset.analyze_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_prompt(text: str, label: str = None):\n",
    "    # Formulate the text with a specific instruction for classification\n",
    "    prompt = \"\"\"\n",
    "You will be given an Arabic hotel review. Your task is to classify it as one of the labels in the list: positive, negative. Output the label only, and nothing else.\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "Answer: {label}\n",
    "\"\"\".strip()\n",
    "    return prompt.format(text=text, label=label)\n",
    "\n",
    "# Mapping dictionary\n",
    "def create_dataset(df):\n",
    "    rows=[]\n",
    "    for _,row in tqdm(df.iterrows()):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"input\": create_prompt(row.text, row.label),\n",
    "                \"output\": row.label,\n",
    "            }\n",
    "        )\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df_pandas_test['label'] = df_pandas_test['label'].replace({1: 'positive', 0: 'negative'})\n",
    "train_rows = create_dataset(df_train)\n",
    "Path(\"mednli_train_data.json\").write_text(json.dumps(train_rows))\n",
    "\n",
    "dev_rows = create_dataset(df_dev)\n",
    "Path(\"mednli_dev_data.json\").write_text(json.dumps(dev_rows))\n",
    "print(dev_rows[0][\"input\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading and Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration Functions ---\n",
    "def setup_tokenizer(model_id: str):\n",
    "    \"\"\"Sets up the tokenizer for the model.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    print(\"Vocabulary size:\", len(tokenizer))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return tokenizer\n",
    "\n",
    "def setup_quantization():\n",
    "    \"\"\"Sets up the quantization configuration.\"\"\"\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, # Enables 4-bit quantization of the base model to reduce memory usage.\n",
    "        bnb_4bit_quant_type=\"nf4\", #  The quantization type (\"nf4\" is a common choice)\n",
    "        bnb_4bit_use_double_quant=True, # Enables nested quantization (double quantization) for further memory reduction.\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # Specifies the data type for computations during training (float16 in this case).\n",
    "    )\n",
    "    return nf4_config\n",
    "\n",
    "def setup_model(model_id: str, nf4_config):\n",
    "    \"\"\"Loads the model with quantization and configuration.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=nf4_config\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    return model\n",
    "\n",
    "def setup_lora(modules):\n",
    "    \"\"\"Sets up the LoRA configuration.\"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=64,  # 8\n",
    "        lora_alpha=16, # 32 Decrease if overfitting is observed or your dataset is small.\n",
    "        target_modules=modules, # check target_modules for any model (see function target_modules)\n",
    "        lora_dropout=0.1, # 0.05 increase (e.g., 0.2-0.3) for small datasets or noisy data.\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    return lora_config\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} || Total params: {all_param} || Trainable%: {100 * trainable_params / all_param:.2f}\")\n",
    "\n",
    "# --- Main Training Function ---\n",
    "\n",
    "def train_model(model, task: str, modules, train_dataset, val_dataset):\n",
    "    \"\"\"Handles the entire training process.\"\"\"\n",
    "    os.environ[\"WANDB_PROJECT\"] = f\"llama32_3B_{task}_3EPs\"\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "    # Apply LoRA\n",
    "    model.enable_input_require_grads()\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print_trainable_parameters(model)\n",
    "    model = model.to(device)\n",
    "    model.hf_device_map\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        args = SFTConfig(\n",
    "            dataset_text_field =\"input\",\n",
    "            # max_seq_length=512,\n",
    "            output_dir=f\"llama32_3B_{task}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\", \n",
    "            # eval_steps = 0.2\n",
    "            learning_rate=2e-4, # 2e-5 learning rate, based on QLoRA paper use 2e-4\n",
    "            per_device_train_batch_size=4, #\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=1,            # number of steps before performing a backward/update pass\n",
    "            # gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            fp16=True,\n",
    "            bf16=False,\n",
    "            num_train_epochs=3, \n",
    "            weight_decay=0.001, # Lower (e.g., 0.001) if underfitting or noisy datasets.\n",
    "            max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "            max_steps=-1,\n",
    "            # group_by_length=False,\n",
    "            lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "            warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=50, # try 1\n",
    "            report_to=\"wandb\",\n",
    "            push_to_hub=True,\n",
    "            packing=False,\n",
    "        ),\n",
    "        peft_config=lora_config,\n",
    "        # dataset_kwargs={\n",
    "        #     \"add_special_tokens\": False,\n",
    "        #     \"append_concat_token\": False\n",
    "        # }\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Training completed in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Building the model\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    login(token=\"your_hf\") # write token\n",
    "    !wandb login your_wandb_token\n",
    "    model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    task = \"ASA\" # Arabic Sentiment Analysis\n",
    "\n",
    "    train_dataset = Dataset.from_list(train_rows)\n",
    "    dev_dataset = Dataset.from_list(dev_rows)\n",
    "\n",
    "    tokenizer = setup_tokenizer(model_id)\n",
    "    nf4_config = setup_quantization()\n",
    "    model = setup_model(model_id, nf4_config)\n",
    "    model = model.to(device)\n",
    "    modules = find_all_linear_names(model)\n",
    "    lora_config = setup_lora(modules)\n",
    "    # train_model(p_model, task, formatting_prompts_func_sst2, modules)\n",
    "    train_model(model, task, modules, train_dataset, dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# One instance\n",
    "%%time\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inp_test = test_rows[0][\"input\"]\n",
    "messages = [{\"role\": \"user\", \"content\": inp_test}]\n",
    "content = messages[0][\"content\"]\n",
    "inputs = tokenizer(content, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1, temperature=0.000001, pad_token_id=tokenizer.eos_token_id)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "parts = [part.strip() for part in generated_text.split(\"Answer:\") if part.strip()]\n",
    "\n",
    "\n",
    "regex = r\"^\\W+|\\W+$\"\n",
    "prediction = re.sub(regex, \"\", parts[-1])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "predictions = []\n",
    "true_values = []\n",
    "for row in tqdm(test_rows):\n",
    "    messages = [{\"role\": \"user\", \"content\": row[\"input\"]}]\n",
    "    content = messages[0][\"content\"]\n",
    "    inputs = tokenizer(content, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1, temperature=0.000001, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    parts = [part.strip() for part in generated_text.split(\"Answer:\") if part.strip()]\n",
    "    predictions.append(parts[-1])\n",
    "    true_values.append(row[\"output\"])\n",
    "\n",
    "regex = r\"^\\W+|\\W+$\"\n",
    "predictions = [re.sub(regex, \"\", p) for p in predictions]\n",
    "len(true_values), len(predictions)\n",
    "pd.Series(predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# if the model generated classes that doesn't exist\n",
    "class_names = ['positive', 'negative']\n",
    "eval_df = pd.DataFrame().from_dict({\"label\":true_values, \"prediction\":predictions})\n",
    "len(eval_df[~eval_df.prediction.isin(class_names)]) # check ...\n",
    "eval_df = eval_df[eval_df.prediction.isin(class_names)] # remove if len > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(true_values, predictions))\n",
    "print(classification_report(eval_df.label, eval_df.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(eval_df.label, eval_df.prediction, labels=class_names)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
